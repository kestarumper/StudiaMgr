{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fname):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "def load_model(fname):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "        return model\n",
    "\n",
    "def test_accurracy(model, dataset):\n",
    "    predicted = model.predict(dataset.data)\n",
    "    return np.mean(predicted == dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test_set = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "train_set.target_names #prints all the categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch stemming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /home/adrian/nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/adrian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('snowball_data')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vectorization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch config (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of parameters and their values to be checked.\n",
    "# All the parameters name are of the form 'stepName__paramName'.\n",
    "# E.g. 'vect__ngram_range': [(1, 1), (1, 2)]\n",
    "# that means use unigram and bigrams and choose the one which is optimal.\n",
    "\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],  \n",
    "    'tfidf__use_idf': [True],\n",
    "    'dtc__n_estimators': [64, 128, 256],\n",
    "    'dtc__max_depth': [32],\n",
    "}\n",
    "\n",
    "pipe_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()), \n",
    "    ('tfidf', TfidfTransformer()), \n",
    "    ('dtc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "gs_clf = GridSearchCV(pipe_clf, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score: 0.7556135236708782\n",
    "# Best param: {'dtc__max_depth': 16, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
    "# Best score: 0.802545504840292\n",
    "# Best param: {'dtc__max_depth': 32, 'dtc__n_estimators': 128, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n",
    "# Best score: 0.8153619542925773\n",
    "# Best param: {'dtc__max_depth': 32, 'dtc__n_estimators': 256, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8153619542925773\n",
      "Best param: {'dtc__max_depth': 32, 'dtc__n_estimators': 256, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "gs_clf = gs_clf.fit(train_set.data, train_set.target)\n",
    "model = gs_clf.best_estimator_\n",
    "\n",
    "print(\"Best score: %s\" % gs_clf.best_score_) \n",
    "print(\"Best param: %s\" % gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"random_forest.pckl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7510621348911312"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_model = load_model(\"random_forest.pckl\")\n",
    "test_accurracy(model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 StemmedCountVectorizer(ngram_range=(1, 2),\n",
       "                                        stop_words='english')),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('dtc',\n",
       "                 RandomForestClassifier(class_weight='balanced',\n",
       "                                        max_depth=100))])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_clf_manual = Pipeline([\n",
    "    ('vect', stemmed_count_vect), \n",
    "    ('tfidf', TfidfTransformer()), \n",
    "    ('dtc', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "pipe_clf_manual.set_params(\n",
    "    vect__ngram_range=(1,2),\n",
    "    tfidf__use_idf=True,\n",
    "    dtc__n_estimators=100,\n",
    "    dtc__max_depth=100,\n",
    "    dtc__class_weight='balanced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manual_random_forest = pipe_clf_manual.fit(train_set.data, train_set.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.789830058417419\n"
     ]
    }
   ],
   "source": [
    "predicted = pipe_clf_manual.predict(test_set.data)\n",
    "\n",
    "print(\"Accuracy: {}\".format(np.mean(predicted == test_set.target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
