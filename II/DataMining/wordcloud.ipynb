{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stemming.porter2 import stem\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWords(fname):\n",
    "    with open(fname, encoding=\"UTF-8\") as file:\n",
    "        return [word for line in file for word in line.split()]\n",
    "\n",
    "stopWords = loadWords(\"stopwords_en.txt\")\n",
    "def filterWords(words):\n",
    "    words = filter(lambda word: len(word) > 0, [transform(word) for word in words])\n",
    "    words = difference(words, stopWords)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(words, stopWords):\n",
    "    return [word for word in words if word not in stopWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(word):\n",
    "    return stem(word.lower().translate(str.maketrans('', '', punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countOccurences(words):\n",
    "    occurences = { word : 1 for word in words }\n",
    "    for word in words:\n",
    "        occurences[word] += 1\n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSVToFile(data, fname):\n",
    "    with open(fname,'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['weight','word'])\n",
    "        csv_out.writerows(data)\n",
    "        \n",
    "def writeToFile(fname, data):\n",
    "    with open(fname, 'w') as out:\n",
    "        out.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideIntoChapters(fname):\n",
    "    pattern = r\"####-\\s*\\n*(Chapter \\d+\\.?(\\s[\\w-]+)*)\"\n",
    "    with open(fname, 'r', encoding='UTF-8') as file:\n",
    "        book = file.read()\n",
    "        chapters = [term[0] for term in re.findall(pattern, book, re.IGNORECASE)]\n",
    "        zipped = zip(chapters, re.split(pattern, book)[0::3][1:])\n",
    "        return { name : content for name, content in zipped}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = filterWords(loadWords(\"lotr.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict = countOccurences(words)\n",
    "pairs = list(wordDict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('frodo', 1992),\n",
       " ('great', 1408),\n",
       " ('long', 1382),\n",
       " ('gandalf', 1305),\n",
       " ('sam', 1289),\n",
       " ('befor', 1233),\n",
       " ('back', 1227),\n",
       " ('dark', 1208),\n",
       " ('time', 1050),\n",
       " ('day', 993)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.sort(key=lambda pair: pair[1], reverse=True); pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = sum(wordDict.values())\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    N = len(documents)\n",
    "    allWords = [word for document in documents for word in document.keys()]\n",
    "    idfDict = dict.fromkeys(allWords, 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(document, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in document.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chapters = divideIntoChapters(\"lotr.txt\");\n",
    "# for name, content in chapters.items():\n",
    "#     writeToFile(\"chapters/\" + name + \".txt\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = \"chapters/\"\n",
    "chapterFileNames = [f for f in listdir(mypath) if isfile(join(mypath, f))];\n",
    "documents = [countOccurences(filterWords(loadWords(mypath + fname))) for fname in chapterFileNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentsTFs = [computeTF(doc) for doc in documents]\n",
    "idfs = computeIDF(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentsTFIDFs = [computeTFIDF(doc, idfs) for doc in documentsTFs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfsToData(document):\n",
    "    return sorted(map(lambda pair: (math.floor(pair[1] * 100000), pair[0]), document.items()), key=lambda pair: pair[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chapterName, document in zip(chapterFileNames, documentsTFIDFs):\n",
    "    data = tfidfsToData(document)\n",
    "    writeCSVToFile(data, 'weighted/' + chapterName + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMerged(documents):\n",
    "    allPairs = [(word, val) for doc in documentsTFIDFs for word, val in doc.items()]\n",
    "    tfidf = dict.fromkeys([word for word, val in allPairs], 0)\n",
    "    for word, val in allPairs:\n",
    "        tfidf[word] += val\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDocuments = getMerged(documentsTFIDFs)\n",
    "writeCSVToFile(tfidfsToData(mergedDocuments), 'wordcloud_tfidf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWord(word):\n",
    "    zipped = dict(zip(chapterFileNames, documentsTFIDFs))\n",
    "    appearIn = [name for name, stats in zipped.items() if word in stats.keys()]\n",
    "    return sorted([(name, math.floor(100000 * zipped[name][word])) for name in appearIn], key=lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chapter 2. The Passage of the Marshes.txt', 2010),\n",
       " ('Chapter 7. Journey to the Cross-roads.txt', 1177),\n",
       " ('Chapter 3. The Black Gate is Closed.txt', 1142),\n",
       " ('Chapter 6. The Forbidden Pool.txt', 1077),\n",
       " ('Chapter 8. The Stairs of Cirith Ungol.txt', 1028),\n",
       " ('Chapter 9. Shelob.txt', 875),\n",
       " ('Chapter 11. The Palantnr.txt', 870),\n",
       " ('Chapter 3. Mount Doom.txt', 380),\n",
       " ('Chapter 2.txt', 296),\n",
       " ('Chapter 2. The Council of Elrond.txt', 222),\n",
       " ('Chapter 9. The Great River.txt', 178),\n",
       " ('Chapter 2. The Land of Shadow.txt', 160),\n",
       " ('Chapter 10. The Breaking of the Fellowship.txt', 63),\n",
       " ('Chapter 3. The Ring Goes South.txt', 56),\n",
       " ('Chapter 4. The Siege of Gondor.txt', 50),\n",
       " ('Chapter 3. The Uruk-Hai.txt', 50),\n",
       " ('Chapter 5. The Window on the West.txt', 40),\n",
       " ('Chapter 10. The Choices of Master Samwise.txt', 31),\n",
       " ('Chapter 10. The Black Gate Opens.txt', 26)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findWord(\"gollum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "allWords = filterWords(loadWords(\"lotr.txt\"))\n",
    "def nextToWord(word):\n",
    "    try:\n",
    "        indices = [i for i, x in enumerate(allWords) if x == word]\n",
    "        wordsAfter = [allWords[i+1] for i in indices]\n",
    "        return list(map(lambda pair: pair[0], sorted(Counter(wordsAfter).items(), key=lambda pair: pair[1], reverse=True)))[:5]\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "next5 = { word : nextToWord(word) for word in set(allWords[:1000]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
